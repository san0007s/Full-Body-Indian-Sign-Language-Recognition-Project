{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b303767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression,RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "801c38ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89910ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_holistic = mp.solutions .holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98673ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer.py\n",
    "import cv2\n",
    "import numpy as np\n",
    "import joblib\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "import mediapipe as mp\n",
    "from collections import deque\n",
    "import time  # for sign‐label debounce\n",
    "\n",
    "# ==== SILENCE ANY HMM WARNINGS ====\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# ====== LOAD ARTIFACTS ======\n",
    "sign_lstm    = load_model('sign_lstm_deeper.keras')\n",
    "sign_le      = joblib.load('sign_label_encoder.pkl')\n",
    "sign_scaler  = joblib.load('sign_scaler.pkl')\n",
    "sign_pca     = joblib.load('sign_pca.pkl')\n",
    "sign_ghmm    = joblib.load('sign_ghmm.pkl')\n",
    "sign_rf      = joblib.load('sign_rf.pkl')\n",
    "\n",
    "feat_extractor = Model(sign_lstm.input,\n",
    "                       sign_lstm.get_layer('feature_reducer').output)\n",
    "\n",
    "face_le     = joblib.load('face_label_encoder.pkl')\n",
    "face_scaler = joblib.load('face_scaler.pkl')\n",
    "face_rf     = joblib.load('face_rf.pkl')\n",
    "\n",
    "# ====== CONFIG ======\n",
    "SEQUENCE_LENGTH      = 30\n",
    "POSE_LM, HAND_LM, FACE_LM = 33, 21, 468\n",
    "SIGN_DISPLAY_DURATION = 5  # seconds to hold last sign\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing  = mp.solutions.drawing_utils\n",
    "\n",
    "# ====== HELPERS ======\n",
    "def extract_landmarks(lm_list, expected_count):\n",
    "    if not lm_list:\n",
    "        return [0.0] * expected_count * 4\n",
    "    vals = []\n",
    "    for lm in lm_list.landmark:\n",
    "        x = lm.x if lm.x == lm.x else 0.0\n",
    "        y = lm.y if lm.y == lm.y else 0.0\n",
    "        z = lm.z if lm.z == lm.z else 0.0\n",
    "        v = lm.visibility if lm.visibility == lm.visibility else 0.0\n",
    "        vals += [x, y, z, v]\n",
    "    # pad if missing\n",
    "    if len(vals) < expected_count * 4:\n",
    "        vals += [0.0] * (expected_count * 4 - len(vals))\n",
    "    return vals\n",
    "\n",
    "# ====== REAL‑TIME LOOP ======\n",
    "sign_buffer      = deque(maxlen=SEQUENCE_LENGTH)\n",
    "last_sign_time   = 0\n",
    "last_sign_label  = \"\"\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# speed up by lowering resolution\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH,  640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.6,\n",
    "                          min_tracking_confidence=0.6) as holistic:\n",
    "    print(\"Starting inference. Press 'q' to quit.\")\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        rgb   = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = holistic.process(rgb)\n",
    "\n",
    "        # draw landmarks (optional)\n",
    "        if results.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, results.pose_landmarks,\n",
    "                                      mp_holistic.POSE_CONNECTIONS)\n",
    "        if results.left_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, results.left_hand_landmarks,\n",
    "                                      mp_holistic.HAND_CONNECTIONS)\n",
    "        if results.right_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, results.right_hand_landmarks,\n",
    "                                      mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "        # extract all landmarks\n",
    "        p  = extract_landmarks(results.pose_landmarks,  POSE_LM)\n",
    "        lh = extract_landmarks(results.left_hand_landmarks, HAND_LM)\n",
    "        rh = extract_landmarks(results.right_hand_landmarks, HAND_LM)\n",
    "        f  = extract_landmarks(results.face_landmarks,  FACE_LM)\n",
    "\n",
    "        # buffer for sign sequence\n",
    "        feats = p + lh + rh\n",
    "        sign_buffer.append(feats)\n",
    "\n",
    "        current_time = time.time()\n",
    "\n",
    "        # run sign pipeline when buffer full\n",
    "        if len(sign_buffer) == SEQUENCE_LENGTH:\n",
    "            Xs = np.array(sign_buffer, dtype=np.float32)[None, ...]\n",
    "            nr, _, nf = Xs.shape\n",
    "            Xflat     = Xs.reshape(-1, nf)\n",
    "            Xs_scaled = sign_scaler.transform(Xflat).reshape(nr, SEQUENCE_LENGTH, nf)\n",
    "\n",
    "            # extract LSTM features\n",
    "            lstmf   = feat_extractor.predict_on_batch(Xs_scaled)\n",
    "            pca_feat = sign_pca.transform(lstmf)\n",
    "            hs       = sign_ghmm.predict(pca_feat).reshape(-1, 1)\n",
    "            comb_feat= np.hstack((lstmf, hs))\n",
    "\n",
    "            pred_enc = sign_rf.predict(comb_feat)\n",
    "            new_label= sign_le.inverse_transform(pred_enc)[0]\n",
    "            # update label if changed or held expired\n",
    "            if new_label != last_sign_label or (current_time - last_sign_time) > SIGN_DISPLAY_DURATION:\n",
    "                last_sign_label = new_label\n",
    "                last_sign_time  = current_time\n",
    "\n",
    "            sign_buffer.clear()\n",
    "\n",
    "        # decide whether to show the last sign\n",
    "        if (current_time - last_sign_time) < SIGN_DISPLAY_DURATION:\n",
    "            sign_label = last_sign_label\n",
    "        else:\n",
    "            sign_label = \"\"\n",
    "\n",
    "        # face‐expression pipeline only if face detected\n",
    "        if results.face_landmarks:\n",
    "            Xf      = np.array(f, dtype=np.float32).reshape(1, -1)\n",
    "            Xf_s    = face_scaler.transform(Xf)\n",
    "            face_enc= face_rf.predict(Xf_s)\n",
    "            face_label = face_le.inverse_transform(face_enc)[0]\n",
    "        else:\n",
    "            face_label = \"\"\n",
    "\n",
    "        # display combined result\n",
    "        display_text = f\"{sign_label or '…'} ({face_label or '…'})\"\n",
    "        cv2.putText(frame, display_text,\n",
    "                    (10, 40), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    1, (0, 255, 0), 2)\n",
    "\n",
    "        cv2.imshow(\"Gesture + Expression Recognition\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
