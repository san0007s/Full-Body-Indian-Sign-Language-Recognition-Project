{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a2786e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Data Collection & Training (train.py)\n",
    "# ================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dropout, BatchNormalization, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# ----- CONFIG -----\n",
    "SEQUENCE_LENGTH = 30\n",
    "SIGN_CSV = 'sign_sequences.csv'\n",
    "FACE_CSV = 'face_expressions.csv'\n",
    "\n",
    "# ----- SIGN (Dynamic) DATA LOADING & PREP -----\n",
    "df_sign = pd.read_csv(SIGN_CSV)\n",
    "y_sign = df_sign['class'].values\n",
    "X_sign = df_sign.drop('class', axis=1).values\n",
    "# reshape to (num_samples, SEQUENCE_LENGTH, features_per_frame)\n",
    "features_per_frame = X_sign.shape[1] // SEQUENCE_LENGTH\n",
    "X_sign = X_sign.reshape(-1, SEQUENCE_LENGTH, features_per_frame)\n",
    "\n",
    "# encode labels\n",
    "le_sign = LabelEncoder()\n",
    "y_sign_enc = le_sign.fit_transform(y_sign)\n",
    "\n",
    "# split train/test\n",
    "X_tr_s, X_te_s, y_tr_s, y_te_s = train_test_split(\n",
    "    X_sign, y_sign_enc, test_size=0.3, random_state=1234, stratify=y_sign_enc\n",
    ")\n",
    "\n",
    "# scale per frame\n",
    "scaler_sign = StandardScaler()\n",
    "X_tr_flat = X_tr_s.reshape(-1, features_per_frame)\n",
    "X_te_flat = X_te_s.reshape(-1, features_per_frame)\n",
    "X_tr_scaled = scaler_sign.fit_transform(X_tr_flat).reshape(X_tr_s.shape)\n",
    "X_te_scaled = scaler_sign.transform(X_te_flat).reshape(X_te_s.shape)\n",
    "\n",
    "# ----- BUILD & TRAIN DEEPER LSTM -----\n",
    "inputs = Input(shape=(SEQUENCE_LENGTH, features_per_frame), name='input')\n",
    "# First LSTM stack (deeper network)\n",
    "x = LSTM(256, return_sequences=True)(inputs)\n",
    "x = Dropout(0.3)(x)\n",
    "x = BatchNormalization()(x)\n",
    "# Second LSTM layer\n",
    "x = LSTM(128, return_sequences=True)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = BatchNormalization()(x)\n",
    "# Third LSTM layer\n",
    "x = LSTM(64, return_sequences=False)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = BatchNormalization()(x)\n",
    "# Feature reducer\n",
    "reduced = Dense(16, activation='linear', name='feature_reducer')(x)\n",
    "\n",
    "# classification head\n",
    "d1 = Dense(32, activation='relu')(reduced)\n",
    "outputs = Dense(len(np.unique(y_sign_enc)), activation='softmax')(d1)\n",
    "\n",
    "model_sign = Model(inputs, outputs)\n",
    "model_sign.compile(\n",
    "    optimizer=Adam(0.001), \n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "# callbacks for robust training\n",
    "early_stop = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(factor=0.5, patience=3)\n",
    "\n",
    "model_sign.fit(\n",
    "    X_tr_scaled, y_tr_s,\n",
    "    validation_data=(X_te_scaled, y_te_s),\n",
    "    epochs=30, batch_size=8,\n",
    "    callbacks=[early_stop, reduce_lr]\n",
    ")\n",
    "\n",
    "# ----- FEATURE EXTRACTION & GHMM -----\n",
    "feat_extractor = Model(inputs, model_sign.get_layer('feature_reducer').output)\n",
    "train_feat = feat_extractor.predict(X_tr_scaled)\n",
    "test_feat = feat_extractor.predict(X_te_scaled)\n",
    "\n",
    "# optional PCA\n",
    "pca = PCA(n_components=16, random_state=42)\n",
    "train_pca = pca.fit_transform(train_feat)\n",
    "test_pca = pca.transform(test_feat)\n",
    "\n",
    "# select best HMM model across dimensions\n",
    "def select_best_n_components(X, n_range, criterion='bic'):\n",
    "    best_score = np.inf\n",
    "    best_model = None\n",
    "    for n in n_range:\n",
    "        try:\n",
    "            m = hmm.GaussianHMM(\n",
    "                n_components=n,\n",
    "                covariance_type='diag',\n",
    "                n_iter=100,\n",
    "                tol=1e-3,\n",
    "                init_params='stmc',\n",
    "                verbose=False,\n",
    "                random_state=42\n",
    "            )\n",
    "            m.fit(X)\n",
    "            ll = m.score(X)\n",
    "            k = (n * n) + 2 * n * X.shape[1] - 1\n",
    "            bic = -2 * ll + k * np.log(len(X))\n",
    "            if bic < best_score:\n",
    "                best_score = bic\n",
    "                best_model = m\n",
    "        except Exception:\n",
    "            continue\n",
    "    return best_model\n",
    "\n",
    "ghmm_sign = select_best_n_components(train_pca, range(1, 25))\n",
    "if ghmm_sign is None:\n",
    "    ghmm_sign = hmm.GaussianHMM(n_components=2,\n",
    "        covariance_type='diag',\n",
    "        n_iter=200,\n",
    "        tol=1e-4,\n",
    "        init_params='',\n",
    "        random_state=42)\n",
    "ghmm_sign.fit(train_pca)\n",
    "\n",
    "# hidden states\n",
    "hs_tr = ghmm_sign.predict(train_pca).reshape(-1, 1)\n",
    "hs_te = ghmm_sign.predict(test_pca).reshape(-1, 1)\n",
    "# combine\n",
    "X_tr_rf = np.hstack((train_feat, hs_tr))\n",
    "X_te_rf = np.hstack((test_feat, hs_te))\n",
    "\n",
    "# random forest on combined features\n",
    "rf_sign = RandomForestClassifier(n_estimators=150, max_depth=20, random_state=1234)\n",
    "rf_sign.fit(X_tr_rf, y_tr_s)\n",
    "y_pred_s = rf_sign.predict(X_te_rf)\n",
    "print('Sign RF acc:', accuracy_score(y_te_s, y_pred_s))\n",
    "print(classification_report(y_te_s, y_pred_s))\n",
    "\n",
    "# ----- FACE (Static) DATA LOAD & TRAIN -----\n",
    "df_face = pd.read_csv(FACE_CSV)\n",
    "y_face = df_face['class'].values\n",
    "X_face = df_face.drop('class', axis=1).values\n",
    "le_face = LabelEncoder()\n",
    "y_face_enc = le_face.fit_transform(y_face)\n",
    "X_tr_f, X_te_f, y_tr_f, y_te_f = train_test_split(\n",
    "    X_face, y_face_enc, test_size=0.3, random_state=1234, stratify=y_face_enc\n",
    ")\n",
    "scaler_face = StandardScaler()\n",
    "X_tr_f_s = scaler_face.fit_transform(X_tr_f)\n",
    "X_te_f_s = scaler_face.transform(X_te_f)\n",
    "\n",
    "# train RF for face\n",
    "rf_face = RandomForestClassifier(n_estimators=150, max_depth=15, random_state=1234)\n",
    "rf_face.fit(X_tr_f_s, y_tr_f)\n",
    "y_pred_f = rf_face.predict(X_te_f_s)\n",
    "print('Face RF acc:', accuracy_score(y_te_f, y_pred_f))\n",
    "print(classification_report(y_te_f, y_pred_f))\n",
    "\n",
    "# ----- SAVE MODELS & ARTIFACTS -----\n",
    "joblib.dump(le_sign, 'sign_label_encoder.pkl')\n",
    "model_sign.save('sign_lstm_deeper.keras')\n",
    "joblib.dump(ghmm_sign, 'sign_ghmm.pkl')\n",
    "joblib.dump(rf_sign, 'sign_rf.pkl')\n",
    "joblib.dump(scaler_sign, 'sign_scaler.pkl')\n",
    "joblib.dump(pca, 'sign_pca.pkl')\n",
    "joblib.dump(le_face, 'face_label_encoder.pkl')\n",
    "joblib.dump(rf_face, 'face_rf.pkl')\n",
    "joblib.dump(scaler_face, 'face_scaler.pkl')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
